# -*- coding: utf-8 -*-
"""fixatch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H1eQNtd4sFqa543qjgQrPuDlyBeSWtET
"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, GlobalAveragePooling2D, Dense, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.datasets import cifar100
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np

(x_train_labeled, y_train_labeled), (x_test, y_test) = cifar100.load_data()
x_train_labeled = x_train_labeled / 255.0
x_test = x_test / 255.0


num_labeled_samples = 2500
random_indices = np.random.choice(len(x_train_labeled), num_labeled_samples, replace=False)
x_train_labeled = x_train_labeled[random_indices]
y_train_labeled = y_train_labeled[random_indices]


(x_train_unlabeled, _), (_, _) = cifar100.load_data()
x_train_unlabeled = x_train_unlabeled / 255.0

def resnet_block(x, filters, kernel_size=3, stride=1):
    y = Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)
    y = BatchNormalization()(y)
    y = ReLU()(y)
    y = Conv2D(filters, kernel_size=kernel_size, strides=1, padding='same')(y)
    y = BatchNormalization()(y)

    if stride > 1:
        x = Conv2D(filters, kernel_size=1, strides=stride, padding='same')(x)
    out = Add()([x, y])
    out = ReLU()(out)
    return out


def build_resnet(input_shape, num_classes):
    inputs = Input(shape=input_shape)
    x = Conv2D(64, kernel_size=7, strides=2, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = resnet_block(x, filters=64)
    x = resnet_block(x, filters=64)
    x = resnet_block(x, filters=128, stride=2)
    x = resnet_block(x, filters=128)
    x = resnet_block(x, filters=256, stride=2)
    x = resnet_block(x, filters=256)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs, outputs)
    return model

input_shape = (32, 32, 3)
num_classes = 100
model = build_resnet(input_shape=input_shape, num_classes=num_classes)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

tau = 0.95


lambda_u = 0.1

import imgaug as ia
from imgaug import augmenters as iaa


def weak_augmentation(images):
    seq = iaa.Sequential([
        iaa.Fliplr(0.5),
        iaa.Affine(rotate=(-10, 10)),
        iaa.GaussianBlur(sigma=(0.0, 1.0)),
        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255))
    ])
    augmented_images = seq(images=images)
    return augmented_images


def strong_augmentation(images):
    seq = iaa.Sequential([
        iaa.Crop(percent=(0, 0.1)),
        iaa.Affine(rotate=(-45, 45)),
        iaa.PerspectiveTransform(scale=(0.01, 0.1))
    ])
    augmented_images = seq(images=images)
    return augmented_images

from sklearn.metrics import accuracy_score
batch_size_labeled = 128
batch_size_unlabeled = 7 * batch_size_labeled

labeled_datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)
unlabeled_datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)

labeled_data_generator = labeled_datagen.flow(x_train_labeled, y_train_labeled, batch_size=batch_size_labeled)
unlabeled_data_generator = unlabeled_datagen.flow(x_train_unlabeled, batch_size=batch_size_unlabeled)

loss_fn_labeled = SparseCategoricalCrossentropy()
loss_fn_unlabeled = SparseCategoricalCrossentropy()
optimizer = Adam(learning_rate=0.001)


num_epochs = 50
steps_per_epoch = num_labeled_samples // batch_size_labeled

threshold = 0.8  # You can adjust this threshold as needed

# ...
# ...
def calculate_accuracy(model, x_test, y_test):
    test_predictions = model.predict(x_test)
    test_predictions = np.argmax(test_predictions, axis=1)
    test_accuracy = accuracy_score(y_test, test_predictions)
    return test_accuracy

# Initialize a list to store accuracies
accuracies = []

for epoch in range(num_epochs):
      for step in range(steps_per_epoch):
        x_batch_labeled, y_batch_labeled = next(labeled_data_generator)
        x_batch_unlabeled = next(unlabeled_data_generator)

        # Weak augmentation and pseudo-labeling for unlabeled batch
        weak_augmented_unlabeled = weak_augmentation(x_batch_unlabeled)
        pseudo_labels = model.predict(weak_augmented_unlabeled)

        # Apply the threshold to generate pseudo-labels
        pseudo_labels_thresholded = (pseudo_labels >= threshold).astype(int)

        # Strong augmentation for unlabeled batch
        strong_augmented_unlabeled = strong_augmentation(x_batch_unlabeled)

        with tf.GradientTape() as tape:
            logits_labeled = model(x_batch_labeled, training=True)
            logits_unlabeled = model(strong_augmented_unlabeled, training=True)

            # Convert the pseudo labels to integer format
            pseudo_labels_int = tf.argmax(pseudo_labels_thresholded, axis=1)

            loss_labeled = loss_fn_labeled(y_batch_labeled, logits_labeled)
            loss_unlabeled = loss_fn_labeled(pseudo_labels_int, logits_unlabeled)

            total_loss = loss_labeled + lambda_u * loss_unlabeled



        gradients = tape.gradient(total_loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
      test_accuracy = calculate_accuracy(model, x_test, y_test)
      accuracies.append(test_accuracy)

      print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

# Print the final test accuracy
final_test_accuracy = accuracies[-1]
print(f"Final Test Accuracy: {final_test_accuracy:.4f}")



# Evaluate the model
# test_loss, test_accuracy = model.evaluate(x_test, y_test)
# print("Test Loss:", test_loss)
# print("Test Accuracy:", test_accuracy)